\section{Introduction}

We have previously shown that although the bootstrap fails in the traditional sense of producing confidence intervals, that with a tweak in methodology and perspective that one can arrive at confidence intervals with reasonable coverage behaviors while retaining faithfulness to the lasso model fit. 

The methodological fix revolves around sampling from the full conditional posterior (FCP) when $\hat{\beta}_j = 0$. By replacing this bootstrap sample with a draw from the FCP, we can avoid having a point mass at 0 as well as obtain bootstrapped CIs that have reasonable widths. This turns out to also be rather robust to the initial assumptions made.

The main hang up with gaining buy in for this interval producing method is that it under covers larger values of $\beta$, presumably the ones the practitioner care the most about. This flaw is inherent to constructing intervals around biased estimators which is why much focus has been placed using the lasso as a conduit for producing inference about the population parameter. 

In the exploration of sampling from the FCP while bootstrapping we naturally arrived at the estimator $z_j = \frac{1}{n}\x_j^T\r_{-j}$ as this is at the core of the FCP and in part determines, along with $\lambda$ where the FCP will be centered. Additionally, in the final penalized regression model, it is this value that the penalty is applied to to arrive at each $\bh_j$. Or in other words, if $\bh_j = P(z_j)$,  $z_j = P^{-1}(\bh_j)$. This raises the natural question, what if instead of bootstrapping the lasso estimates we sample the unpenalized estimator. This initially appears like it should perform well and is very simple to estimate. In the non-orthogonal case, $z_j = \beta^*_j + \frac{1}{n}\x_j^T \epsilon + \frac{1}{n}\x_j^T \X_{-j}(\bb^*_{-j} - \bbh_{-j})$. Under the assumption of iid errors, this suggests that $z_j$ is normally distributed around $\beta^*_j$ plus some bias that depends on the correlation between $\x_j$ and the other covariates and the bias of the estimates for $\bb_{-j}$. That is, the direct bias from penalization is removed. To understand the bias that is still introduced, consider wlg a situation where p = 2, $\beta_1 > 0$, $\beta_2 = 0$ and $\bh_1 > 0$, and $\frac{1}{n}\x_1^T\x_2 = \rho > 0$. $z_1$ will be biased towards zero then if $\bh_2 > 0$. If the sample correlation is low or the estimates are reasonably close to the true $\beta$ values, then this bias should be minimal. Additionally, one might expect that the bootstrapping this estimator in high dimensions would help capture this uncertainty about the estimate leading to a certain level of robustness.

However, even in the case of simple independence, the coverage rate is below nominal and is not robust to alternative correlation structures among the predictors. It appears that bootstrapping is rather susceptible to this correlation induced bias. Even just a simple normal approximation using the expression above gives better coverage performance. The normal approximation intervals are also generally narrower, suggesting that the issue with the bootstrap revolves around how it is affected by bias.

\newpage

\section{Another alternative method}

\as{
  \begin{aligned}
  &\text{Normal Approx (used to construct posts in man 1): } &z_j|\bb_{-j} \sim N(\beta_j, \frac{\sigma^2}{x_j^T x_j} ) \\
  &\text{Pipe: } &\bar{\beta}_j | S_j \sim N(\beta_j, \frac{\sigma^2}{x_j^T Q_{S_j} x_j}) \\
  &\text{Relaxed: } &\hat{\beta_j}^* | S_j \sim N(\beta_j, \frac{\sigma^2}{x_j^T Q_{S_j} x_j}) \\
  \end{aligned}
}

Where $z_j = \bar{\beta}_j = \frac{x_j^T(\y - \X_{-j}\hat{\beta}_{-j})}{x_j^T x_j}$, and $\hat{\beta_j}^* = \frac{x_j^T Q_{S_j} \y}{x_j^T Q_{S_j} x_j}$

With the relaxed idea in mind, the debiased lasso is notably an extension of this while also retaining a similarity to PIPE. Specifically, the debiased lasso uses the residuals from the original fit but instead of projecting $\x_j$ onto the active features as the relaxed does, this is where the method gets more involved by using additional lasso fits to project $\x_j$ onto the other features.

\newpage

\section{Inherent bias in bootsrapping}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_ortho_script}
    \caption{\label{Fig:sim_ortho} Covariates generated under orthogonality, SNR = 1, with 8 true non-zero $\beta$ values of decreasing magnitude. Plot displays results from all 1000 simulations.}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_independence}
    \caption{\label{Fig:sim_independence} Covariates generated under independence, SNR = 1, with 8 true non-zero $\beta$ values of decreasing magnitude. Plot displays results from all 1000 simulations.}
    \end{center}
\end{figure}

The normal approximation still undercovers for nonzero $\beta$s but to a lesser degree. If we look at some example CIs for one of the simulation in the figure above, we can see it appears to be less suceptible to the bias.

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=0.65\linewidth]{sim_independence_example}
    \caption{\label{Fig:sim_independence_exaple} An example of results from a single simulation in plot 1.}
    \end{center}
\end{figure}

We can even hone in a bit further and look at the CIs compared to the original debiased estimate compared to the bootstrap draws for debiased:

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=0.65\linewidth]{bootdist_indep}
    \caption{\label{Fig:bootdist_indep} The bootstrap distribution for a for variable 2 in plot 2 with CIs superimposed. }
    \end{center}
\end{figure}

The phenomenon is not unique to this situation of independence either, even as correlation among the predictors is introduced, a place where the bootstrap would be expected to perform better than a normal approximation, this same pattern is still observed. We also see that the normal approximation suffers from bias as correlation among the predictors increases as well. So something more would be needed to make these interval estimates robust to correlation. 

While not the motivation for PIPE, as it turns out, an adjustment to this normal approximation was proposed by \logan{enter citation}. The main idea behind pipe is constructing a test statistic by using approximate projection onto the column space of the active features. The computing the PIPE estimator, $\bar{\beta}$, is equivalent to computing $z_j$, however in its construction, an alternative variance is suggested. Specifically, if $\hat{S} = \lbrace k: \hat{\beta}_k \neq  0 \rbrace$ and $\hat{S}_j = \hat{S} \text{ if } j \notin \hat{S}$ and $\hat{S}_j = \hat{S} - \lbrace j \rbrace \text{ if } j \in \hat{S}$  then let $\Q_{\hat{S}_j} = \I - \X_{\hat{S}_j}(\X_{\hat{S}_j}^T \X_{\hat{S}_j})^{-1} \X_{\hat{S}_j}^T$. The variance of $\bar{\beta}$ is $(\x_j^T \Q_{\hat{S}_j} \x_j)^{-1}\hat{\sigma}^2$. The logical interpretation of $\x_j^T \Q_{\hat{S}_j} \x_j$ is an adjusted sample size for inference on $\beta_j$ based on how much information in $\x_j$ is orthogonal to $\X_{\hat{S}_j}$. 

The beauty of this construction is that the variance of the PIPE estimator is inherently related to the bias of the estimator. So, while no attempt is made to correct for the bias, in cases where the bias is likely large, so too is the resulting variance.

\newpage

\section{Digging into bootstrap bias}

\subsection{Single signal variable, no correlation}

This also just suggests (as is the case for the rest of the decomps) that looking at the difference is the most notable pattern.

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp_orig}
    \caption{\label{Fig:bias_decomp_orig} n = p = 100, $\beta_A = 2$ all other $\beta$s = 0. All covariates generated under independence}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp_boot}
    \caption{\label{Fig:bias_decomp_boot} n = p = 100, $\beta_A = 2$ all other $\beta$s = 0. All covariates generated under independence}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp}
    \caption{\label{Fig:bias_decomp} n = p = 100, $\beta_A = 2$ all other $\beta$s = 0. All covariates generated under independence}
    \end{center}
\end{figure}

\newpage

\subsection{More complex decompositions}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp_single_B}
    \caption{\label{Fig:bias_decomp_single_B} n = p = 100, $\beta_A = 2$, $\beta_B = 0$, $\rho_{A,B} = 0.5$.  All other $\beta$s = 0 and  generated under independence}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp_2_groups}
    \caption{\label{Fig:bias_decomp_2_groups} n = p = 100, $\beta_{A1} = \beta_{A2} = 2$, $\beta_{B1} = \beta{B2} = 0$, $\rho_{\beta{A_i},\beta{B_j}} = 0.5 \text{ if } i=j, \text{ otherwise } \rho_{\beta{A_i},\beta{B_j}} = 0$.  All other $\beta$s = 0 and  generated under independence}
    \end{center}
\end{figure}

\newpage

\subsection{Correlated signals}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp_same_corr_signal}
    \caption{\label{Fig:bias_decomp_same_corr_signal} n = p = 100, $\beta_{A1} = \beta_{B1} = 2$, $\rho_{\beta{A_1},\beta{B_1}} = 0.5$.  All other $\beta$s = 0 and  generated under independence}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp_opp_signal}
    \caption{\label{Fig:bias_decomp_opp_signal} n = p = 100, $\beta_{A1} = -\beta_{B1} = 2$, $\rho_{\beta{A_1},\beta{B_1}} = 0.5$.  All other $\beta$s = 0 and  generated under independence}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{bias_decomp_highcorr}
    \caption{\label{Fig:bias_decomp_highcorr} n = p = 100, $\beta_{A1} = 2, \beta_{B1} = 0$, $\rho_{\beta{A_1},\beta{B_1}} = 0.99$.  All other $\beta$s = 0 and  generated under independence}
    \end{center}
\end{figure}

\newpage

\section{Other results}

\subsection{Opposite direction}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_grp_opposite_dir}
    \caption{\label{Fig:sim_grp_opposite_dir} n = p = 100, group sizes of 5, $\rho_{within} = 0.5$, $\rho_{between} = 0$. $V1_G1 = -V2_G1 = V3_G1 = 0.5$, and all other coefficients are 0.}
    \end{center}
\end{figure}

\begin{table}[hb]
  \centering
  \input{tab/sim_grp_opposite_dir_selection_freq}
  \caption{\label{Tab:sim_grp_opposite_dir_selection_freq}Add a description}
\end{table}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_grp_opposite_dir_balanced}
    \caption{\label{Fig:sim_grp_opposite_dir_balanced} n = p = 100, group sizes of 5, $\rho_{within} = 0.5$, $\rho_{between} = 0$. $V1_G1 = -V2_G1 = V3_G1 = -V4_G1 = 0.5$, and all other coefficients are 0.}
    \end{center}
\end{figure}

\begin{table}[hb]
    \centering
    \input{tab/sim_grp_opposite_dir_balanced_selection_freq}
    \caption{\label{Tab:sim_grp_opposite_dir_balanced_selection_freq}Add a description}
  \end{table}

\subsection{Small Betas (low selection Frequency)}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_small}
    \caption{\label{Fig:sim_small} Covariates generated under independence with 10 true non-zero $\beta$ values = 0.5. n = 50, p = 100. Plot displays results from all 1000 simulations.}
    \end{center}
\end{figure}


\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_abn}
    \caption{\label{Fig:sim_abn} Data generated with the same true values of $\beta$ as under the previous simulation, with now each non-zero $\beta$ is correlated with two null variables with exchangeable correlation and $\rho = 0.5$.}
    \end{center}
\end{figure}


\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_gene}
    \caption{\label{Fig:sim_gene} Covariates generated with AR corr with rho = 0.7, with 10 true non-zero $\beta$ values of decreasing magnitude all contiguous. Plot displays results from all 1000 simulations.}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_ldpe}
    \caption{\label{Fig:sim_ldpe} p = 3000, n = 200, $\beta_j = \sqrt{\frac{18\log(p)}{n}} \text{ if } j \in \lbrace 1500, 1800, 2100, 2400, 2700, 3000 \rbrace$ else $\beta_j = \sqrt{\frac{18\log(p)}{nj^2}}$. Correlation is autoregressive with $\rho = 0.8$}
    \end{center}
\end{figure}


\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_highcorr}
    \caption{\label{Fig:sim_highcorr} n = p = 100, $\rho_{A1, B1} = 0.99$}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_grp_same_dir}
    \caption{\label{Fig:sim_grp_same_dir} n = p = 100, $\rho_{V1, V2} = 0.5$ for $G1$ otherwise, $\rho_{V1, V2} = 0$. $V1_G1 = 0.8$, $V2_G1 = 0.2$, and all other coefficients are 0.}
    \end{center}
\end{figure}

\begin{figure}[hbtp]
    \begin{center}
    \includegraphics[width=\linewidth]{sim_grp_same_dir_same_eff}
    \caption{\label{Fig:ssim_grp_same_dir_same_eff} n = p = 100, group sizes of 4, $\rho_{G1} = 0.5$ but zero for all other groups and $\rho_{between} = 0$. $V1_G1 = V2_G1 = V3_G1 = 1$, and all other coefficients are 0.}
    \end{center}
\end{figure}

\newpage
\section{Average coverage boot alternatives}

As noted previously, when bootstrapping the lasso, the previously recommended fix was to sample from the FCP when $\bh_j = 0$. As one could imagine, throwing out the bootstrap and just constructing intervals from these posteriors results in insufficient coverage. Unsurprising as this serves as a short cut to a fully Bayesian approach. In efforts to obtain computational efficiency, the intervals are too narrow as they do not account for the fact that the FCPs are being constructed conditioning on $\bb_{-j} = \bbh_{-j}$. Doing results in $L(\beta_j|\bbh_{-j}, \lambda, \sigma^2) \propto \exp(-\frac{n}{2\sigma^2}(\beta_{j}^2 - 2 z_{j} \beta_{j}))$.

PIPE can be interpreted as taking this a step further by accounting for the fact that we are conditioning on the selected model, specifically $\hat{S}_j$. 

\logan{Well kinda, is more like a partially conditional likelihood? Or a profile likelihood based on the kkt conditions? I haven't had luck in deriving it yet.}

This results in the following likelihood: $L(\beta_j|\bbh_{-j}, \hat{S}_j, \lambda, \sigma^2) \propto \exp(-\frac{\x_j^T \Q_{\hat{S}_j} \x_j}{2\sigma^2}(\beta_{j}^2 - 2 \bar{\beta}_{j} \beta_{j}))$. 

Building off this likelihood instead of the fully conditional one, we can build posterior distributions that are adjusted to account for the conditioning on the selected model.

In terms of the average coverage properties outlined in \logan{cite first paper}, it behaves very similarly to the proposed methodological fix. However, it works in about a fraction of the time and is more robust to correlation among predictors due to its construction. 

However, it is the application to the MCP penalty that is of greatest interest. Specifically to use this as an alternative option to the PIPE estimator itself.
\begin{table}[hbtp]
    \centering
    \input{tab/distribution_table}
    \caption{\label{Tab:dist_beta} PIPE Based Posterior}
  \end{table}

\begin{table}[hbtp]
\centering
\input{tab/distribution_table_relaxed}
\caption{\label{Tab:dist_beta_relaxed} Relaxed Lasso Based Posterior}
\end{table}